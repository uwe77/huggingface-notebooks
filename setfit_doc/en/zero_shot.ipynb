{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your class names are likely already good descriptors of the text that you're looking to classify. With ðŸ¤— SetFit, you can use these class names with strong pretrained Sentence Transformer models to get a strong baseline model without any training samples.\n",
    "\n",
    "This guide will show you how to perform zero-shot text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the [dair-ai/emotion](https://huggingface.co/datasets/dair-ai/emotion) dataset to test the performance of our zero-shot model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test_dataset = load_dataset(\"dair-ai/emotion\", \"split\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset stores the class names within the dataset `Features`, so we'll extract the classes like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = test_dataset.features[\"label\"].names\n",
    "# => ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, we could manually set the list of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use [get_templated_dataset()](https://huggingface.co/docs/setfit/main/en/reference/utility#setfit.get_templated_dataset) to synthetically generate a dummy dataset given these class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import get_templated_dataset\n",
    "\n",
    "train_dataset = get_templated_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "# => Dataset({\n",
    "#     features: ['text', 'label'],\n",
    "#     num_rows: 48\n",
    "# })\n",
    "print(train_dataset[0])\n",
    "# {'text': 'This sentence is sadness', 'label': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this dataset to train a SetFit model just like normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import SetFitModel, Trainer, TrainingArguments\n",
    "\n",
    "model = SetFitModel.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    batch_size=32,\n",
    "    num_epochs=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "***** Running training *****\n",
    "  Num examples = 60\n",
    "  Num epochs = 1\n",
    "  Total optimization steps = 60\n",
    "  Total train batch size = 32\n",
    "{'embedding_loss': 0.2628, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.02}                                                                                 \n",
    "{'embedding_loss': 0.0222, 'learning_rate': 3.7037037037037037e-06, 'epoch': 0.83}                                                                                 \n",
    "{'train_runtime': 15.4717, 'train_samples_per_second': 124.098, 'train_steps_per_second': 3.878, 'epoch': 1.0}                                                     \n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:09<00:00,  6.35it/s]\n",
    "```\n",
    "\n",
    "Once trained, we can evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "***** Running evaluation *****\n",
    "{'accuracy': 0.591}\n",
    "```\n",
    "\n",
    "And run predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([\n",
    "    \"i am just feeling cranky and blue\",\n",
    "    \"i feel incredibly lucky just to be able to talk to her\",\n",
    "    \"you're pissing me off right now\",\n",
    "    \"i definitely have thalassophobia, don't get me near water like that\",\n",
    "    \"i did not see that coming at all\",\n",
    "])\n",
    "print([classes[idx] for idx in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['sadness', 'joy', 'anger', 'fear', 'surprise']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions all look right!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that the zero-shot performance of SetFit works well, we'll compare it against a zero-shot classification model from `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "# Prepare the testing dataset\n",
    "test_dataset = load_dataset(\"dair-ai/emotion\", \"split\", split=\"test\")\n",
    "classes = test_dataset.features[\"label\"].names\n",
    "\n",
    "# Set up the zero-shot classification pipeline from transformers\n",
    "# Uses 'facebook/bart-large-mnli' by default\n",
    "pipe = pipeline(\"zero-shot-classification\", device=0)\n",
    "zeroshot_preds = pipe(test_dataset[\"text\"], batch_size=16, candidate_labels=classes)\n",
    "preds = [classes.index(pred[\"labels\"][0]) for pred in zeroshot_preds]\n",
    "\n",
    "# Compute the accuracy\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "transformers_accuracy = metric.compute(predictions=preds, references=test_dataset[\"label\"])\n",
    "print(transformers_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'accuracy': 0.3765}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With its 59.1% accuracy, the 0-shot SetFit heavily outperforms the recommended zero-shot model by `transformers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond getting higher accuracies, SetFit is much faster too. Let's compute the latency of SetFit with `BAAI/bge-small-en-v1.5` versus the latency of `transformers` with `facebook/bart-large-mnli`. Both tests were performed on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_t = time.time()\n",
    "pipe(test_dataset[\"text\"], batch_size=32, candidate_labels=classes)\n",
    "delta_t = time.time() - start_t\n",
    "print(f\"`transformers` with `facebook/bart-large-mnli` latency: {delta_t / len(test_dataset['text']) * 1000:.4f}ms per sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "`transformers` with `facebook/bart-large-mnli` latency: 31.1765ms per sentence\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_t = time.time()\n",
    "model.predict(test_dataset[\"text\"])\n",
    "delta_t = time.time() - start_t\n",
    "print(f\"SetFit with `BAAI/bge-small-en-v1.5` latency: {delta_t / len(test_dataset['text']) * 1000:.4f}ms per sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "SetFit with `BAAI/bge-small-en-v1.5` latency: 0.4600ms per sentence\n",
    "```\n",
    "\n",
    "So, SetFit with `BAAI/bge-small-en-v1.5` is 67x faster than `transformers` with `facebook/bart-large-mnli`, alongside being more accurate:\n",
    "\n",
    "![zero_shot_transformers_vs_setfit](https://github.com/huggingface/setfit/assets/37621491/33f574d9-c51b-4e02-8d98-6e04e18427ef)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
